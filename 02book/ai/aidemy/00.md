Aidemy学習ノート

初めて聞く単語やポイントとなる単語をまずまとめてあります。

Index

- 深層学習（ディープラーニング）
    - 1 深層学習の実践
    - 2 深層学習のチューニング

--------------------------

# 深層学習（ディープラーニング）

--------------------------

## 1 深層学習の実践

## 1.1.1 深層学習を体験してみよう

Tnsorflow + Keras

ニューラルネットワーク
ラッパー
エポック数


## 1.1.2 深層学習とは(1)

特徴を見つける話

## 1.1.3 深層学習とは(2)

重みパラメータ
x1w1 + x2w2 -> しきい値より高い　or 低い -> 1 or 0

ディープニューラルネットワーク（DNN)

## 1.1.4 深層学習を用いた分類の流れ

Xを入力として受取、Yを出力する。

正解データ（教師ラベル）T　との　差△E

誤差逆伝播法

分類や回帰のモデル


## 1.2.1 分類までの流れ


## 1.2.2 ディープニューラルネットワーク

全結合相 入力層 出力層 隠れ層

ノード

one-hotベクトル

## 1.2.3 Kerasの導入

ラッパーライブラリ

## 1.2.4 データの用意

MNIST/データセット

from *** import ***

**.shape


## 1.2.5 モデルの生成

model = Sequential()

ユニット数128の全結合相
model.add(Dense(128))

活性化関数
    シグモイド関数 ReLU関数
    sigmoid relu

    model.add(Activation("sigmoid"))


コンパイルメソッド

    compile()



## 1.2.6 モデルの学習

訓練データ

model.fit(X_train,y_train, verbose=1)

進捗度具合の表示　verbose

モデルの予測精度の向上

正解率 acc:



## 1.2.7 モデルの評価

モデルのチューニング

モデルの評価

汎化精度の計算

score = model.evalute(X_test, y_test, verbose=1)


## 1.2.8 モデルによる分類

predメソッド 予測値の取得

pred = model.predict(X_test[0]
print("predict" + str(pred))

argmax関数


--------------------------
## 2 深層学習のチューニング

## 2.1.1 ハイパーパラメータ

ハイパーパラメータ

活性化関数
隠れ層の数、隠れ層のチャンネル数
ドロップ・アウトする割合
学習率 Ir
最適化関数
誤差関数
バッチサイズ
エポック数


## 2.2.1 ネットワーク構造

精度の出るモデル
隠れ層の構造がモデル学習に与える影響


## 2.3.1 ドロップアウト

過学習 0で上書き

model.add(Dropout(rate=0.5))


## 2.4.1 活性化関数

線形分離不可能


## 2.4.2 シグモイド関数

導関数


## 2.4.3 ReLU


## 2.5.1 損失関数

損失関数（誤差関数）
二乗誤差
クロスエントロピー誤差

誤差逆伝播法



## 2.5.2 二乗誤差

## 2.5.3 クロスエントロピー誤差

## 2.6.1 最適化関数

## ß2.7.1 学習率

最小化


## 2.8.1 ミニバッチ学習

バッチサイズ
損失関数の勾配

局所解

癖の強いデータが多い時はバッチサイズを大きくする、
同じようなデータが多いときはバッチサイズを小さくする

オンライン学習(確率的勾配法)
バッチ学習（再急降下法）
ミニバッチ学習




#＃ 2.9.1 反復学習

エポック数





--------------------------------


- 概念的には

実践とチューニングがあって、全ては正解率 acc:　を上げるために
モデルの予測精度を上げる事が必要なのだが、過学習やら元のデータの性質に応じたチューニングが必要という理解。

- 感想としては

概念はなんとなくわかったが、これそのまま利用できるかというとちょっと・・・・

- ここから先

ここにあるのをすすめるのがいいのか・・・？
https://aidemy.net/courses



